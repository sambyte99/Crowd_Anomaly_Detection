{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crowd Anomaly Detection .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs3y4cKPagtN",
        "colab_type": "text"
      },
      "source": [
        "#---------------------**CROWD ANOMALY DETECTION**---------------------\n",
        "\n",
        "> ### Presented by \n",
        "> # Vishakha Bhat and Sambit Sanyal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1rizLM2Fvzf",
        "colab_type": "text"
      },
      "source": [
        "># ****DISCLAIMER****\n",
        "> ### This code is best run using Google colab. Thats where it was tried and tested\n",
        "\n",
        "> #### The code should run fine in any new Google colab project as long as the right uploads are done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqLC34h8OvNs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> ## *Please ignore this if you are working with Google collab.*\n",
        "\n",
        "> ## Before you run this code, some installations need to be done:\n",
        "---\n",
        "> ### `!pip install numpy` to install numpy\n",
        "\n",
        "> ### `!pip install sklearn` to install sklearn\n",
        "\n",
        "> ### `!pip install Keras` to install keras\n",
        "\n",
        "> ### `!pip install tensorflow` to install tensorflow , But ofcourse for model training its recommended you use the GPU version so install \n",
        "\n",
        "> ### `!pip install tensorflow-gpu` \n",
        "\n",
        "> ### `!pip install h5py` to install h5py\n",
        "\n",
        "> ### `!pip install scipy` to install scipy\n",
        "\n",
        "> ### `!pip install skimage` to install skimage\n",
        "\n",
        "> ### `!pip install ffmpeg` to install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGV2WeuK4ST",
        "colab_type": "text"
      },
      "source": [
        "# **STEP 1)** \n",
        "> ## So First lets create the `trainer.npy` with the help of the videos and the datasets. Please upload the Avenue training dataset and set the directory location in the code.\n",
        "\n",
        "---\n",
        "> ## In the likely secenario where you cannot find the Avenue Dataset. \n",
        "> ## [Please look for it in this link.](http://www.cse.cuhk.edu.hk/leojia/projects/detectabnormal/dataset.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p84NQE0WQiVZ",
        "colab_type": "text"
      },
      "source": [
        "> ## So I shall uploading the data and keeping it in a folder called \n",
        "> ## `training_videos` and shall be copying the directory path into the code.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfrrcA1EOx07",
        "colab_type": "code",
        "outputId": "fd089d81-38d6-44cf-e226-9c976241f271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "'''\n",
        "Hello. Word of advice. Please ensure you check the variable video_source_path refers to the folder with the dataset of training \n",
        "and also make sure you have uploaded the correct training videos and not the testing videos\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import img_to_array,load_img\n",
        "import numpy as np\n",
        "import glob\n",
        "import os \n",
        "from skimage import data, color\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import argparse\n",
        "from PIL import Image\n",
        "imagestore=[]\n",
        "\n",
        "\n",
        "\n",
        "video_source_path='/content/training_videos'\n",
        "fps=5\n",
        "#fps refers to the number of seconds after which one frame will be taken . fps=5 means 1 frame after every 5 seconds. More like seconds per frame.\n",
        "\n",
        "def create_dir(path):\n",
        "\tif not os.path.exists(path):\n",
        "\t\tos.makedirs(path)\n",
        "\n",
        "def remove_old_images(path):\n",
        "\tfilelist = glob.glob(os.path.join(path, \"*.png\"))\n",
        "\tfor f in filelist:\n",
        "\t\tos.remove(f)\n",
        "\n",
        "def store(image_path):\n",
        "\timg=load_img(image_path)\n",
        "\timg=img_to_array(img)\n",
        "\n",
        "\n",
        "\t#Resize the Image to (227,227,3) for the network to be able to process it. \n",
        "\n",
        "\n",
        "\timg=resize(img,(227,227,3))\n",
        "\n",
        "\t#Convert the Image to Grayscale\n",
        "\n",
        "\n",
        "\tgray=0.2989*img[:,:,0]+0.5870*img[:,:,1]+0.1140*img[:,:,2]\n",
        "\n",
        "\timagestore.append(gray)\n",
        "\n",
        "\n",
        "\n",
        "#List of all Videos in the Source Directory.\n",
        "videos=os.listdir(video_source_path)\n",
        "print(\"Found \",len(videos),\" training videos\")\n",
        "\n",
        "\n",
        "#Make a temp dir to store all the frames\n",
        "create_dir(video_source_path+'/frames')\n",
        "\n",
        "#Remove old images\n",
        "remove_old_images(video_source_path+'/frames')\n",
        "\n",
        "framepath=video_source_path+'/frames'\n",
        "\n",
        "for video in videos:\n",
        "\t\tos.system( 'ffmpeg -i {}/{} -r 1/{}  {}/frames/%03d.jpg'.format(video_source_path,video,fps,video_source_path))\n",
        "\t\timages=os.listdir(framepath)\n",
        "\t\tfor image in images:\n",
        "\t\t\timage_path=framepath+ '/'+ image\n",
        "\t\t\tstore(image_path)\n",
        "\n",
        "\n",
        "imagestore=np.array(imagestore)\n",
        "a,b,c=imagestore.shape\n",
        "#Reshape to (227,227,batch_size)\n",
        "imagestore.resize(b,c,a)\n",
        "#Normalize\n",
        "imagestore=(imagestore-imagestore.mean())/(imagestore.std())\n",
        "#Clip negative Values\n",
        "imagestore=np.clip(imagestore,0,1)\n",
        "np.save('trainer.npy',imagestore)\n",
        "#Remove Buffer Directory\n",
        "os.system('rm -r {}'.format(framepath))\n",
        "print(\"Program ended. Please wait while trainer.npy is created. \\nRefresh when needed\")\n",
        "print('Number of frames created :', int(len(imagestore)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found  16  training videos\n",
            "Program ended. Please wait while trainer.npy is created. \n",
            "Refresh when needed\n",
            "Number of frames created : 227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR78ZawmaZJ6",
        "colab_type": "text"
      },
      "source": [
        " >## So right now a new model trainer called the `trainer.npy` should have been created! \n",
        "\n",
        ">## Please confirm its existence before you jump into the next section. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y7OVrdeBBuJ",
        "colab_type": "text"
      },
      "source": [
        "# **STEP 2)**   \n",
        "> ## Now that `trainer.npy` is created , we can run the below code and train the model using it.\n",
        "\n",
        "> ## The model so created will be called `AnomalyDetector.h5`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6VM2ZTfdRTH",
        "colab_type": "code",
        "outputId": "5e72ee21-1d5e-4a03-87d7-da556ca37d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import numpy as np \n",
        "import argparse\n",
        "from keras.layers import Conv3D,ConvLSTM2D,Conv3DTranspose\n",
        "from keras.models import Sequential\n",
        "\n",
        "''' The following load_model function code has been taken from \n",
        "Abnormal Event Detection in Videos using Spatiotemporal Autoencoder\n",
        "by Yong Shean Chong Yong Haur Tay\n",
        "Lee Kong Chian Faculty of Engineering Science, Universiti Tunku Abdul Rahman, 43000 Kajang, Malaysia.\n",
        "It's main purpose is to help us generate the anomaly detector model\n",
        "'''\n",
        "\n",
        "#load_model starts here :----------------------------------------------------\n",
        "def load_model():\n",
        "\t\"\"\"\n",
        "\tReturn the model used for abnormal event \n",
        "\tdetection in videos using spatiotemporal autoencoder\n",
        "\n",
        "\t\"\"\"\n",
        "\tmodel=Sequential()\n",
        "\tmodel.add(Conv3D(filters=128,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',input_shape=(227,227,10,1),activation='tanh'))\n",
        "\tmodel.add(Conv3D(filters=64,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
        "\n",
        "\n",
        "\n",
        "\tmodel.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,padding='same',dropout=0.4,recurrent_dropout=0.3,return_sequences=True))\n",
        "\n",
        "\t\n",
        "\tmodel.add(ConvLSTM2D(filters=32,kernel_size=(3,3),strides=1,padding='same',dropout=0.3,return_sequences=True))\n",
        "\n",
        "\n",
        "\tmodel.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,return_sequences=True, padding='same',dropout=0.5))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tmodel.add(Conv3DTranspose(filters=128,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
        "\tmodel.add(Conv3DTranspose(filters=1,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',activation='tanh'))\n",
        "\n",
        "\tmodel.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "\n",
        "#load_model ends here :----------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "X_train=np.load('trainer.npy')\n",
        "frames=X_train.shape[2]\n",
        "#Need to make number of frames divisible by 10 to ease the load_model\n",
        "\n",
        "\n",
        "frames=frames-frames%10\n",
        "\n",
        "X_train=X_train[:,:,:frames]\n",
        "X_train=X_train.reshape(-1,227,227,10)\n",
        "X_train=np.expand_dims(X_train,axis=4)\n",
        "Y_train=X_train.copy()\n",
        "\n",
        "\n",
        "epochs=200\n",
        "batch_size=1\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "\n",
        "\tmodel=load_model()\n",
        "\n",
        "\tcallback_save = ModelCheckpoint(\"AnomalyDetector.h5\",\n",
        "\t\t\t\t\t\t\t\t\tmonitor=\"mean_squared_error\")\n",
        "\n",
        "\tcallback_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "\tprint('Trainer has been loaded')\n",
        "\tmodel.fit(X_train,Y_train,\n",
        "\t\t\t  batch_size=batch_size,\n",
        "\t\t\t  epochs=epochs,\n",
        "\t\t\t  callbacks = [callback_save,callback_early_stopping]\n",
        "\t\t\t  )\n",
        "print('Done\\n Please wait while AnomalyDetector.h5 has been created \\nRefresh when needed')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainer has been loaded\n",
            "Epoch 1/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.2402 - accuracy: 0.5296\n",
            "Epoch 2/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 47s 2s/step - loss: 0.2002 - accuracy: 0.5491\n",
            "Epoch 3/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.1813 - accuracy: 0.5735\n",
            "Epoch 4/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.1196 - accuracy: 0.6793\n",
            "Epoch 5/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0880 - accuracy: 0.7185\n",
            "Epoch 6/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0802 - accuracy: 0.7257\n",
            "Epoch 7/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0757 - accuracy: 0.7315\n",
            "Epoch 8/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0733 - accuracy: 0.7344\n",
            "Epoch 9/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0716 - accuracy: 0.7353\n",
            "Epoch 10/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0695 - accuracy: 0.7364\n",
            "Epoch 11/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0685 - accuracy: 0.7372\n",
            "Epoch 12/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0682 - accuracy: 0.7374\n",
            "Epoch 13/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0673 - accuracy: 0.7383\n",
            "Epoch 14/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0657 - accuracy: 0.7393\n",
            "Epoch 15/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0649 - accuracy: 0.7401\n",
            "Epoch 16/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0636 - accuracy: 0.7415\n",
            "Epoch 17/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0627 - accuracy: 0.7425\n",
            "Epoch 18/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0619 - accuracy: 0.7429\n",
            "Epoch 19/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0615 - accuracy: 0.7437\n",
            "Epoch 20/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0609 - accuracy: 0.7442\n",
            "Epoch 21/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0601 - accuracy: 0.7447\n",
            "Epoch 22/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0594 - accuracy: 0.7451\n",
            "Epoch 23/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0584 - accuracy: 0.7460\n",
            "Epoch 24/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0578 - accuracy: 0.7468\n",
            "Epoch 25/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0571 - accuracy: 0.7473\n",
            "Epoch 26/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0563 - accuracy: 0.7479\n",
            "Epoch 27/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0553 - accuracy: 0.7486\n",
            "Epoch 28/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0546 - accuracy: 0.7491\n",
            "Epoch 29/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0532 - accuracy: 0.7500\n",
            "Epoch 30/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0527 - accuracy: 0.7508\n",
            "Epoch 31/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0502 - accuracy: 0.7532\n",
            "Epoch 32/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0482 - accuracy: 0.7554\n",
            "Epoch 33/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0463 - accuracy: 0.7574\n",
            "Epoch 34/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0455 - accuracy: 0.7588\n",
            "Epoch 35/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0428 - accuracy: 0.7617\n",
            "Epoch 36/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0408 - accuracy: 0.7641\n",
            "Epoch 37/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0388 - accuracy: 0.7662\n",
            "Epoch 38/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0376 - accuracy: 0.7680\n",
            "Epoch 39/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0367 - accuracy: 0.7690\n",
            "Epoch 40/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0357 - accuracy: 0.7701\n",
            "Epoch 41/200\n",
            "22/22 [==============================] - 54s 2s/step - loss: 0.0349 - accuracy: 0.7708\n",
            "Epoch 42/200\n",
            "22/22 [==============================] - 50s 2s/step - loss: 0.0337 - accuracy: 0.7718\n",
            "Epoch 43/200\n",
            "22/22 [==============================] - 50s 2s/step - loss: 0.0331 - accuracy: 0.7725\n",
            "Epoch 44/200\n",
            "22/22 [==============================] - 50s 2s/step - loss: 0.0335 - accuracy: 0.7728\n",
            "Epoch 45/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0319 - accuracy: 0.7736\n",
            "Epoch 46/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0311 - accuracy: 0.7742\n",
            "Epoch 47/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0305 - accuracy: 0.7746\n",
            "Epoch 48/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0302 - accuracy: 0.7750\n",
            "Epoch 49/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0300 - accuracy: 0.7751\n",
            "Epoch 50/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0293 - accuracy: 0.7756\n",
            "Epoch 51/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0289 - accuracy: 0.7758\n",
            "Epoch 52/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0287 - accuracy: 0.7760\n",
            "Epoch 53/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0284 - accuracy: 0.7764\n",
            "Epoch 54/200\n",
            "22/22 [==============================] - 52s 2s/step - loss: 0.0277 - accuracy: 0.7767\n",
            "Epoch 55/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0274 - accuracy: 0.7769\n",
            "Epoch 56/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0271 - accuracy: 0.7771\n",
            "Epoch 57/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0271 - accuracy: 0.7772\n",
            "Epoch 58/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0269 - accuracy: 0.7775\n",
            "Epoch 59/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0262 - accuracy: 0.7777\n",
            "Epoch 60/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0260 - accuracy: 0.7779\n",
            "Epoch 61/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0261 - accuracy: 0.7780\n",
            "Epoch 62/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0260 - accuracy: 0.7782\n",
            "Epoch 63/200\n",
            "22/22 [==============================] - 47s 2s/step - loss: 0.0255 - accuracy: 0.7784\n",
            "Epoch 64/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0252 - accuracy: 0.7787\n",
            "Epoch 65/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0248 - accuracy: 0.7790\n",
            "Epoch 66/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0245 - accuracy: 0.7793\n",
            "Epoch 67/200\n",
            "22/22 [==============================] - 51s 2s/step - loss: 0.0242 - accuracy: 0.7795\n",
            "Epoch 68/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0241 - accuracy: 0.7797\n",
            "Epoch 69/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0239 - accuracy: 0.7799\n",
            "Epoch 70/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0236 - accuracy: 0.7802\n",
            "Epoch 71/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0234 - accuracy: 0.7804\n",
            "Epoch 72/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0231 - accuracy: 0.7806\n",
            "Epoch 73/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0232 - accuracy: 0.7807\n",
            "Epoch 74/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0228 - accuracy: 0.7810\n",
            "Epoch 75/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0230 - accuracy: 0.7811\n",
            "Epoch 76/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0227 - accuracy: 0.7813\n",
            "Epoch 77/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0222 - accuracy: 0.7815\n",
            "Epoch 78/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0220 - accuracy: 0.7817\n",
            "Epoch 79/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0219 - accuracy: 0.7818\n",
            "Epoch 80/200\n",
            "22/22 [==============================] - 50s 2s/step - loss: 0.0218 - accuracy: 0.7819\n",
            "Epoch 81/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0216 - accuracy: 0.7821\n",
            "Epoch 82/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0214 - accuracy: 0.7823\n",
            "Epoch 83/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0213 - accuracy: 0.7825\n",
            "Epoch 84/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0213 - accuracy: 0.7826\n",
            "Epoch 85/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0210 - accuracy: 0.7828\n",
            "Epoch 86/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0209 - accuracy: 0.7829\n",
            "Epoch 87/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0209 - accuracy: 0.7830\n",
            "Epoch 88/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0205 - accuracy: 0.7832\n",
            "Epoch 89/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0204 - accuracy: 0.7833\n",
            "Epoch 90/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0203 - accuracy: 0.7833\n",
            "Epoch 91/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0201 - accuracy: 0.7835\n",
            "Epoch 92/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0200 - accuracy: 0.7836\n",
            "Epoch 93/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0203 - accuracy: 0.7837\n",
            "Epoch 94/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0202 - accuracy: 0.7837\n",
            "Epoch 95/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0198 - accuracy: 0.7839\n",
            "Epoch 96/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0198 - accuracy: 0.7840\n",
            "Epoch 97/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0196 - accuracy: 0.7841\n",
            "Epoch 98/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0195 - accuracy: 0.7841\n",
            "Epoch 99/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0194 - accuracy: 0.7842\n",
            "Epoch 100/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0194 - accuracy: 0.7843\n",
            "Epoch 101/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0191 - accuracy: 0.7845\n",
            "Epoch 102/200\n",
            "22/22 [==============================] - 50s 2s/step - loss: 0.0190 - accuracy: 0.7845\n",
            "Epoch 103/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0190 - accuracy: 0.7846\n",
            "Epoch 104/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0189 - accuracy: 0.7847\n",
            "Epoch 105/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0189 - accuracy: 0.7847\n",
            "Epoch 106/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0187 - accuracy: 0.7848\n",
            "Epoch 107/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0188 - accuracy: 0.7849\n",
            "Epoch 108/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0186 - accuracy: 0.7849\n",
            "Epoch 109/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0185 - accuracy: 0.7850\n",
            "Epoch 110/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0185 - accuracy: 0.7851\n",
            "Epoch 111/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0182 - accuracy: 0.7851\n",
            "Epoch 112/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0182 - accuracy: 0.7852\n",
            "Epoch 113/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0180 - accuracy: 0.7853\n",
            "Epoch 114/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0181 - accuracy: 0.7853\n",
            "Epoch 115/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0180 - accuracy: 0.7854\n",
            "Epoch 116/200\n",
            "22/22 [==============================] - 49s 2s/step - loss: 0.0178 - accuracy: 0.7855\n",
            "Epoch 117/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0179 - accuracy: 0.7854\n",
            "Epoch 118/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0178 - accuracy: 0.7856\n",
            "Epoch 119/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0178 - accuracy: 0.7856\n",
            "Epoch 120/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0178 - accuracy: 0.7857\n",
            "Epoch 121/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0175 - accuracy: 0.7857\n",
            "Epoch 122/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0174 - accuracy: 0.7858\n",
            "Epoch 123/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0174 - accuracy: 0.7858\n",
            "Epoch 124/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0172 - accuracy: 0.7859\n",
            "Epoch 125/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0172 - accuracy: 0.7859\n",
            "Epoch 126/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0172 - accuracy: 0.7859\n",
            "Epoch 127/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0171 - accuracy: 0.7860\n",
            "Epoch 128/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0171 - accuracy: 0.7861\n",
            "Epoch 129/200\n",
            "22/22 [==============================] - 50s 2s/step - loss: 0.0170 - accuracy: 0.7861\n",
            "Epoch 130/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0170 - accuracy: 0.7861\n",
            "Epoch 131/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0169 - accuracy: 0.7862\n",
            "Epoch 132/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0167 - accuracy: 0.7862\n",
            "Epoch 133/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0168 - accuracy: 0.7863\n",
            "Epoch 134/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0166 - accuracy: 0.7863\n",
            "Epoch 135/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0166 - accuracy: 0.7863\n",
            "Epoch 136/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0166 - accuracy: 0.7864\n",
            "Epoch 137/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0165 - accuracy: 0.7864\n",
            "Epoch 138/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0164 - accuracy: 0.7864\n",
            "Epoch 139/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0166 - accuracy: 0.7864\n",
            "Epoch 140/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0164 - accuracy: 0.7865\n",
            "Epoch 141/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0164 - accuracy: 0.7865\n",
            "Epoch 142/200\n",
            "22/22 [==============================] - 48s 2s/step - loss: 0.0162 - accuracy: 0.7866\n",
            "Epoch 143/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0162 - accuracy: 0.7866\n",
            "Epoch 144/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0162 - accuracy: 0.7866\n",
            "Epoch 145/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0162 - accuracy: 0.7867\n",
            "Epoch 146/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0161 - accuracy: 0.7867\n",
            "Epoch 147/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0162 - accuracy: 0.7867\n",
            "Epoch 148/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0160 - accuracy: 0.7868\n",
            "Epoch 149/200\n",
            "22/22 [==============================] - 39s 2s/step - loss: 0.0158 - accuracy: 0.7869\n",
            "Epoch 150/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0158 - accuracy: 0.7869\n",
            "Epoch 151/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0158 - accuracy: 0.7869\n",
            "Epoch 152/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0157 - accuracy: 0.7869\n",
            "Epoch 153/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0159 - accuracy: 0.7869\n",
            "Epoch 154/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0156 - accuracy: 0.7870\n",
            "Epoch 155/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0155 - accuracy: 0.7870\n",
            "Epoch 156/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0155 - accuracy: 0.7871\n",
            "Epoch 157/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0155 - accuracy: 0.7871\n",
            "Epoch 158/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0155 - accuracy: 0.7871\n",
            "Epoch 159/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0155 - accuracy: 0.7871\n",
            "Epoch 160/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0156 - accuracy: 0.7872\n",
            "Epoch 161/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0153 - accuracy: 0.7872\n",
            "Epoch 162/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0154 - accuracy: 0.7872\n",
            "Epoch 163/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0154 - accuracy: 0.7873\n",
            "Epoch 164/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0152 - accuracy: 0.7873\n",
            "Epoch 165/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0152 - accuracy: 0.7873\n",
            "Epoch 166/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0152 - accuracy: 0.7873\n",
            "Epoch 167/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0151 - accuracy: 0.7873\n",
            "Epoch 168/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0152 - accuracy: 0.7873\n",
            "Epoch 169/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0151 - accuracy: 0.7874\n",
            "Epoch 170/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0150 - accuracy: 0.7874\n",
            "Epoch 171/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0149 - accuracy: 0.7875\n",
            "Epoch 172/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0150 - accuracy: 0.7874\n",
            "Epoch 173/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0150 - accuracy: 0.7875\n",
            "Epoch 174/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0148 - accuracy: 0.7875\n",
            "Epoch 175/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0148 - accuracy: 0.7875\n",
            "Epoch 176/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0148 - accuracy: 0.7875\n",
            "Epoch 177/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0147 - accuracy: 0.7876\n",
            "Epoch 178/200\n",
            "22/22 [==============================] - 42s 2s/step - loss: 0.0147 - accuracy: 0.7876\n",
            "Epoch 179/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0146 - accuracy: 0.7877\n",
            "Epoch 180/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0146 - accuracy: 0.7877\n",
            "Epoch 181/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0146 - accuracy: 0.7876\n",
            "Epoch 182/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0147 - accuracy: 0.7877\n",
            "Epoch 183/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0147 - accuracy: 0.7877\n",
            "Epoch 184/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0147 - accuracy: 0.7877\n",
            "Epoch 185/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0146 - accuracy: 0.7877\n",
            "Epoch 186/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0145 - accuracy: 0.7878\n",
            "Epoch 187/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0145 - accuracy: 0.7878\n",
            "Epoch 188/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0144 - accuracy: 0.7878\n",
            "Epoch 189/200\n",
            "22/22 [==============================] - 43s 2s/step - loss: 0.0144 - accuracy: 0.7878\n",
            "Epoch 190/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0143 - accuracy: 0.7878\n",
            "Epoch 191/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0144 - accuracy: 0.7878\n",
            "Epoch 192/200\n",
            "22/22 [==============================] - 44s 2s/step - loss: 0.0143 - accuracy: 0.7879\n",
            "Epoch 193/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0143 - accuracy: 0.7879\n",
            "Epoch 194/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0142 - accuracy: 0.7879\n",
            "Epoch 195/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0141 - accuracy: 0.7879\n",
            "Epoch 196/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0142 - accuracy: 0.7879\n",
            "Epoch 197/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0141 - accuracy: 0.7879\n",
            "Epoch 198/200\n",
            "22/22 [==============================] - 45s 2s/step - loss: 0.0141 - accuracy: 0.7880\n",
            "Epoch 199/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0140 - accuracy: 0.7880\n",
            "Epoch 200/200\n",
            "22/22 [==============================] - 46s 2s/step - loss: 0.0139 - accuracy: 0.7880\n",
            "Done\n",
            " Please wait while AnomalyDetector.h5 has been created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4kx504uVqjs",
        "colab_type": "text"
      },
      "source": [
        "> ### **If this is taking too long , please feel free to take this already ready trained model**\n",
        ">  ### It has been with epoches as 200. Its final accuracy stands at 0.7880\n",
        "> # [Download it from here](https://drive.google.com/drive/folders/1AbF68y7ofutgZObca8D4K6Z33s66GDCP?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d47Qmn3OuC8_",
        "colab_type": "text"
      },
      "source": [
        "> ## Please do not run the next code untill you can confirm that `AnomalyDetector.h5` has been successfully created and its most accurate version has been updated. It takes a while. You can come back a century later.\n",
        "\n",
        "\n",
        "---\n",
        "**Please note that for demonstration of training the model please reduce the epoches to lower values. Running high value of epoches on CPU is not recommended.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQakgwDqV5S3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "> ## Right now there is no longer a compulsion to continue the execution in Google colab. The files ```trainer.npy``` and ```AnomalyDetector.h5``` are enough to enable testing the videos in CPU.\n",
        "\n",
        "\n",
        "> ## However for the completion of this document as a full fledged project the code below can be used to run on testing videos \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgOllDlN-Xmp",
        "colab_type": "text"
      },
      "source": [
        "#**STEP 3)** \n",
        "> ## Upload the \"Avenue Dataset\" testing Videos in the folder called the `testing_videos`\n",
        "\n",
        "---\n",
        "> ## For testing the videos we need to create a `tester.npy` and run the trained model on it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ1RUgfXlVho",
        "colab_type": "text"
      },
      "source": [
        "> ## Make sure you have the `AnomalyDetector.h5` in the main folder otherwise you shall ofcourse get some errors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8sIcLnqEEPN",
        "colab_type": "code",
        "outputId": "109c374f-f370-418f-f70d-855aaf3b6fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "Hello. Word of advice. Please ensure you check the variable video_source_path refers to the folder with the dataset of testing videos\n",
        "and also make sure you have uploaded the correct testing videos and not the training videos\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import img_to_array,load_img\n",
        "import numpy as np\n",
        "import glob\n",
        "import os \n",
        "from skimage import data, color\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import argparse\n",
        "from PIL import Image\n",
        "imagestore=[]\n",
        "\n",
        "\n",
        "\n",
        "video_source_path='/content/testing_videos'\n",
        "fps=5\n",
        "#fps refers to the number of seconds after which one frame will be taken . fps=5 means 1 frame after every 5 seconds. More like seconds per frame.\n",
        "\n",
        "def create_dir(path):\n",
        "\tif not os.path.exists(path):\n",
        "\t\tos.makedirs(path)\n",
        "\n",
        "def remove_old_images(path):\n",
        "\tfilelist = glob.glob(os.path.join(path, \"*.png\"))\n",
        "\tfor f in filelist:\n",
        "\t\tos.remove(f)\n",
        "\n",
        "def store(image_path):\n",
        "\timg=load_img(image_path)\n",
        "\timg=img_to_array(img)\n",
        "\n",
        "\n",
        "\t#Resize the Image to (227,227,3) for the network to be able to process it. \n",
        "\n",
        "\n",
        "\timg=resize(img,(227,227,3))\n",
        "\n",
        "\t#Convert the Image to Grayscale\n",
        "\n",
        "\n",
        "\tgray=0.2989*img[:,:,0]+0.5870*img[:,:,1]+0.1140*img[:,:,2]\n",
        "\n",
        "\timagestore.append(gray)\n",
        "#List of all Videos in the Source Directory.\n",
        "videos=os.listdir(video_source_path)\n",
        "print(\"Found \",len(videos),\" testing videos\")\n",
        "\n",
        "\n",
        "#Make a temp dir to store all the frames\n",
        "create_dir(video_source_path+'/frames')\n",
        "\n",
        "#Remove old images\n",
        "remove_old_images(video_source_path+'/frames')\n",
        "\n",
        "framepath=video_source_path+'/frames'\n",
        "total=0\n",
        "video_count=0\n",
        "\n",
        "for video in videos:\n",
        "\t\tvideo_count+=1\n",
        "\t\tprint(\"Video number: \",video_count)\n",
        "\t\tprint(\"Video:\",str(video))\n",
        "\t\timage_count=0\n",
        "\t\tos.system( 'ffmpeg -i {}/{} -r 1/{}  {}/frames/%03d.jpg'.format(video_source_path,video,fps,video_source_path))\n",
        "\t\timages=os.listdir(framepath)\n",
        "\t\timage_count=len(images)\n",
        "\t\tfor image in images:\n",
        "\t\t\timage_path=framepath+ '/'+ image\n",
        "\t\t\tstore(image_path)\n",
        "\t\ttotal=len(images)+total\n",
        "\t\tprint(\"Number of images:\",image_count,\"\\n----------\\n\")\n",
        "\n",
        "\n",
        "imagestore=np.array(imagestore)\n",
        "a,b,c=imagestore.shape\n",
        "#Reshape to (227,227,batch_size)\n",
        "imagestore.resize(b,c,a)\n",
        "#Normalize\n",
        "imagestore=(imagestore-imagestore.mean())/(imagestore.std())\n",
        "#Clip negative Values\n",
        "imagestore=np.clip(imagestore,0,1)\n",
        "np.save('tester.npy',imagestore)\n",
        "#Remove Buffer Directory\n",
        "os.system('rm -r {}'.format(framepath))\n",
        "\n",
        "print(\"Program ended. All testing videos shall be stored in tester.npy \\n Please wait while tester.npy is created. \\nRefresh when needed\")\n",
        "print('Number of frames created :', int(total))\n",
        "print ('Number of bunches=',int(total),\"/10 = \",int(total/10))\n",
        "print(\"\\nCorrupted and unreadable bunches were ignored\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found  22  testing videos\n",
            "Video number:  1\n",
            "Video: 03.avi\n",
            "Number of images: 9 \n",
            "----------\n",
            "\n",
            "Video number:  2\n",
            "Video: 01.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  3\n",
            "Video: 05.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  4\n",
            "Video: 15.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  5\n",
            "Video: 02.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  6\n",
            "Video: 13.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  7\n",
            "Video: 16.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  8\n",
            "Video: 09.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  9\n",
            "Video: 10.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  10\n",
            "Video: 17.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  11\n",
            "Video: 04.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  12\n",
            "Video: 06.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  13\n",
            "Video: .ipynb_checkpoints\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  14\n",
            "Video: 07.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  15\n",
            "Video: 20.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  16\n",
            "Video: 14.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  17\n",
            "Video: 21.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  18\n",
            "Video: 08.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  19\n",
            "Video: 18.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  20\n",
            "Video: 11.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  21\n",
            "Video: 19.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Video number:  22\n",
            "Video: 12.avi\n",
            "Number of images: 13 \n",
            "----------\n",
            "\n",
            "Program ended. All testing videos shall be stored in tester.npy \n",
            " Please wait while tester.npy is created. \n",
            "Refresh when needed\n",
            "Number of frames created : 282\n",
            "Number of bunches= 282 /10 =  28\n",
            "\n",
            "Corrupted and unreadable bunches were ignored\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxqZLuz8FpnP",
        "colab_type": "text"
      },
      "source": [
        "#**STEP 4)** \n",
        "> ## Right now wait for the `tester.npy` to generate and then run the below code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zENbe-9XHSvo",
        "colab_type": "text"
      },
      "source": [
        "> ## Some errors may creep in the dataset, but they will be removed in the program because they will be corrupted and unreadable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcfYMukPeOx6",
        "colab_type": "code",
        "outputId": "8cf2da32-6688-4e75-9571-34035a5ef09c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "from keras.models import load_model\n",
        "import numpy as np \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mean_squared_loss(x1,x2):\n",
        "\n",
        "\n",
        "\t''' Compute Euclidean Distance Loss  between \n",
        "\tinput frame and the reconstructed frame'''\n",
        "\n",
        "\n",
        "\tdiff=x1-x2\n",
        "\ta,b,c,d,e=diff.shape\n",
        "\tn_samples=a*b*c*d*e\n",
        "\tsq_diff=diff**2\n",
        "\tSum=sq_diff.sum()\n",
        "\tdist=np.sqrt(Sum)\n",
        "\tmean_dist=dist/n_samples\n",
        "\n",
        "\treturn mean_dist\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''Define threshold for Sensitivity\n",
        "Lower the Threshhold,higher the chances that a bunch of frames will be flagged as Anomalous.\n",
        "\n",
        "'''\n",
        "\n",
        "#threshold=0.0004 #(Accuracy level 1)\n",
        "#threshold=0.00042 #(Accuracy level 2)\n",
        "threshold=0.0008#(Accuracy level Vishakha)\n",
        "\n",
        "model=load_model('AnomalyDetector.h5')\n",
        "\n",
        "X_test=np.load('tester.npy')\n",
        "frames=X_test.shape[2]\n",
        "#Need to make number of frames divisible by 10\n",
        "\n",
        "\n",
        "flag=0 #Overall video flagq\n",
        "\n",
        "frames=frames-frames%10\n",
        "\n",
        "X_test=X_test[:,:,:frames]\n",
        "X_test=X_test.reshape(-1,227,227,10)\n",
        "X_test=np.expand_dims(X_test,axis=4)\n",
        "counter =0\n",
        "for number,bunch in enumerate(X_test):\n",
        "\tn_bunch=np.expand_dims(bunch,axis=0)\n",
        "\treconstructed_bunch=model.predict(n_bunch)\n",
        "\n",
        "\n",
        "\tloss=mean_squared_loss(n_bunch,reconstructed_bunch)\n",
        "\t\n",
        "\tif loss>threshold:\n",
        "\t\tprint(\"Anomalous bunch of frames at bunch number {}\".format(number))\n",
        "\t\tcounter=counter+1\n",
        "\t\tprint(\"bunch number: \",counter)\n",
        "\t\tflag=1\n",
        "\n",
        "\n",
        "\telse:\n",
        "\t\tprint('No anomaly')\n",
        "\t\tcounter=counter+1\n",
        "\t\tprint(\"bunch number: \",counter)\n",
        "\n",
        "\n",
        "\n",
        "if flag==1:\n",
        "\tprint(\"Anomalous Events detected\")\n",
        "else:\n",
        "\tprint(\"No anomaly detected\")\n",
        "\t\n",
        "print(\"\\nCorrupted and unreadable bunches were ignored\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No anomaly\n",
            "bunch number:  1\n",
            "No anomaly\n",
            "bunch number:  2\n",
            "No anomaly\n",
            "bunch number:  3\n",
            "Anomalous bunch of frames at bunch number 3\n",
            "bunch number:  4\n",
            "Anomalous bunch of frames at bunch number 4\n",
            "bunch number:  5\n",
            "No anomaly\n",
            "bunch number:  6\n",
            "No anomaly\n",
            "bunch number:  7\n",
            "No anomaly\n",
            "bunch number:  8\n",
            "No anomaly\n",
            "bunch number:  9\n",
            "No anomaly\n",
            "bunch number:  10\n",
            "No anomaly\n",
            "bunch number:  11\n",
            "No anomaly\n",
            "bunch number:  12\n",
            "No anomaly\n",
            "bunch number:  13\n",
            "No anomaly\n",
            "bunch number:  14\n",
            "No anomaly\n",
            "bunch number:  15\n",
            "No anomaly\n",
            "bunch number:  16\n",
            "No anomaly\n",
            "bunch number:  17\n",
            "Anomalous bunch of frames at bunch number 17\n",
            "bunch number:  18\n",
            "Anomalous bunch of frames at bunch number 18\n",
            "bunch number:  19\n",
            "Anomalous bunch of frames at bunch number 19\n",
            "bunch number:  20\n",
            "No anomaly\n",
            "bunch number:  21\n",
            "No anomaly\n",
            "bunch number:  22\n",
            "No anomaly\n",
            "bunch number:  23\n",
            "No anomaly\n",
            "bunch number:  24\n",
            "No anomaly\n",
            "bunch number:  25\n",
            "No anomaly\n",
            "bunch number:  26\n",
            "No anomaly\n",
            "bunch number:  27\n",
            "No anomaly\n",
            "bunch number:  28\n",
            "Anomalous Events detected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQYet1pcJ_4E",
        "colab_type": "text"
      },
      "source": [
        "> ## Now to run the code on chosen files, we have to run the following code . Please set the video file location in the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRQCjdFGEuPG",
        "colab_type": "text"
      },
      "source": [
        "> ## Please upload the `test.mp4` or `test.avi` as a testing video. Please ensure that the video isnt doctored and edited. It should be continous stream of frames\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhBwtxuwKwQ8",
        "colab_type": "code",
        "outputId": "85a0b476-8eba-459c-de12-5520aecacceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "'''\n",
        "Hello. Word of advice. Please ensure you check the variable video_source_path refers to the folder with the dataset of training \n",
        "and also make sure you have uploaded the correct training videos and not the testing videos\n",
        "\n",
        "\n",
        "'''\n",
        "from keras.models import load_model\n",
        "import numpy as np \n",
        "\n",
        "from keras.preprocessing.image import img_to_array,load_img\n",
        "import numpy as np\n",
        "import glob\n",
        "import os \n",
        "from skimage import data, color\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import argparse\n",
        "from PIL import Image\n",
        "imagestore=[]\n",
        "\n",
        "\n",
        "\n",
        "video_source_path='/content/'\n",
        "fps=5\n",
        "#fps refers to the number of seconds after which one frame will be taken . fps=5 means 1 frame after every 5 seconds. More like seconds per frame.\n",
        "\n",
        "def create_dir(path):\n",
        "\tif not os.path.exists(path):\n",
        "\t\tos.makedirs(path)\n",
        "\n",
        "def remove_old_images(path):\n",
        "\tfilelist = glob.glob(os.path.join(path, \"*.png\"))\n",
        "\tfor f in filelist:\n",
        "\t\tos.remove(f)\n",
        "\n",
        "def store(image_path):\n",
        "\timg=load_img(image_path)\n",
        "\timg=img_to_array(img)\n",
        "\n",
        "\n",
        "\t#Resize the Image to (227,227,3) for the network to be able to process it. \n",
        "\n",
        "\n",
        "\timg=resize(img,(227,227,3))\n",
        "\n",
        "\t#Convert the Image to Grayscale\n",
        "\n",
        "\n",
        "\tgray=0.2989*img[:,:,0]+0.5870*img[:,:,1]+0.1140*img[:,:,2]\n",
        "\n",
        "\timagestore.append(gray)\n",
        "\n",
        "\n",
        "\n",
        "#List of all Videos in the Source Directory.\n",
        "videos=os.listdir(video_source_path)\n",
        "print(\"Found \",len(videos),\" files\")\n",
        "\n",
        "\n",
        "#Make a temp dir to store all the frames\n",
        "create_dir(video_source_path+'/frames')\n",
        "\n",
        "#Remove old images\n",
        "remove_old_images(video_source_path+'/frames')\n",
        "\n",
        "framepath=video_source_path+'/frames'\n",
        "flag=0\n",
        "for video in videos:\n",
        "\t\tif (video==\"test.avi\" or video==\"test.mp4\"):\n",
        "\t\t\tprint(\"Test video found\")\n",
        "\t\t\tflag=1\n",
        "\t\t\tos.system( 'ffmpeg -i {}/{} -r 1/{}  {}/frames/%03d.jpg'.format(video_source_path,video,fps,video_source_path))\n",
        "\t\t\timages=os.listdir(framepath)\n",
        "\t\t\tfor image in images:\n",
        "\t\t\t\timage_path=framepath+ '/'+ image\n",
        "\t\t\t\tstore(image_path)\n",
        "\n",
        "if flag==0:\n",
        "\tprint(\"Couldn't find test.mp4 or test.avi. Make sure you reupload and try this\")\n",
        "\texit()\n",
        "imagestore=np.array(imagestore)\n",
        "a,b,c=imagestore.shape\n",
        "#Reshape to (227,227,batch_size)\n",
        "imagestore.resize(b,c,a)\n",
        "#Normalize\n",
        "imagestore=(imagestore-imagestore.mean())/(imagestore.std())\n",
        "#Clip negative Values\n",
        "imagestore=np.clip(imagestore,0,1)\n",
        "np.save('sample.npy',imagestore)\n",
        "#Remove Buffer Directory\n",
        "os.system('rm -r {}'.format(framepath))\n",
        "print(\"Please wait while video is processed. \\nRefresh when needed\")\n",
        "\n",
        "\n",
        "def mean_squared_loss(x1,x2):\n",
        "\n",
        "\n",
        "\t''' Compute Euclidean Distance Loss  between \n",
        "\tinput frame and the reconstructed frame'''\n",
        "\n",
        "\n",
        "\tdiff=x1-x2\n",
        "\ta,b,c,d,e=diff.shape\n",
        "\tn_samples=a*b*c*d*e\n",
        "\tsq_diff=diff**2\n",
        "\tSum=sq_diff.sum()\n",
        "\tdist=np.sqrt(Sum)\n",
        "\tmean_dist=dist/n_samples\n",
        "\n",
        "\treturn mean_dist\n",
        "\n",
        "\n",
        "'''Define threshold for Sensitivity\n",
        "Lower the Threshhold,higher the chances that a bunch of frames will be flagged as Anomalous.\n",
        "\n",
        "'''\n",
        "\n",
        "#threshold=0.0004 #(Accuracy level 1)\n",
        "#threshold=0.00042 #(Accuracy level 2)\n",
        "threshold=0.0008#(Accuracy level 3)\n",
        "\n",
        "model=load_model('AnomalyDetector.h5')\n",
        "\n",
        "X_test=np.load('sample.npy')\n",
        "frames=X_test.shape[2]\n",
        "#Need to make number of frames divisible by 10\n",
        "\n",
        "\n",
        "flag=0 #Overall video flagq\n",
        "\n",
        "frames=frames-frames%10\n",
        "\n",
        "X_test=X_test[:,:,:frames]\n",
        "X_test=X_test.reshape(-1,227,227,10)\n",
        "X_test=np.expand_dims(X_test,axis=4)\n",
        "counter =0\n",
        "for number,bunch in enumerate(X_test):\n",
        "\tn_bunch=np.expand_dims(bunch,axis=0)\n",
        "\treconstructed_bunch=model.predict(n_bunch)\n",
        "\n",
        "\n",
        "\tloss=mean_squared_loss(n_bunch,reconstructed_bunch)\n",
        "\t\n",
        "\tif loss>threshold:\n",
        "\t\tprint(\"Anomalous bunch of frames at bunch number {}\".format(number))\n",
        "\t\tcounter=counter+1\n",
        "\t\tprint(\"bunch number: \",counter)\n",
        "\t\tflag=1\n",
        "\n",
        "\n",
        "\telse:\n",
        "\t\tprint('No anomaly')\n",
        "\t\tcounter=counter+1\n",
        "\t\tprint(\"bunch number: \",counter)\n",
        "\n",
        "\n",
        "print(\"----------------------------------------------------\\nOUTPUT\\n----------------------------------------------------\\n\")\n",
        "if flag==1:\n",
        "\tprint(\"Anomalous Events detected\")\n",
        "else:\n",
        "\tprint(\"No anomaly detected\")\n",
        "\t\n",
        "print(\"\\n----------------------------------------------------\\nCorrupted and unreadable bunches were ignored\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found  6  files\n",
            "Test video found\n",
            "Please wait while video is processed. \n",
            "Refresh when needed\n",
            "Anomalous bunch of frames at bunch number 0\n",
            "bunch number:  1\n",
            "----------------------------------------------------\n",
            "OUTPUT\n",
            "----------------------------------------------------\n",
            "\n",
            "Anomalous Events detected\n",
            "\n",
            "----------------------------------------------------\n",
            "Corrupted and unreadable bunches were ignored\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvAdKj0j6nu",
        "colab_type": "text"
      },
      "source": [
        "# **WE ARE DONE**\n",
        "> ## Yipee. That ends our project.\n",
        "> ## Queries and comments shall be addressed later I guess."
      ]
    }
  ]
}